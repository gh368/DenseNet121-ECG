[file_names_and_paths]
; Output model filename
model_name = DenseNet121_Transformer
; Path to the dataset folder (containing the h5py files)
dataset_folder = data/ptb-xl/official_splits

[encoder_parameters]
; Number of blocks in the encoder (18 or 34)
num_blocks = 34
; Kernel sizes for the convolutional layers
kernel_size = [9, 7, 7, 5]
; Kernel size for the initial stem layer
stem_kernel_size = 9
; Size of the encoded ECG feature vector (temporal features)
encoded_ecg_size = 1

[decoder_parameters]
; Number of layers in the Transformer decoder
num_layers = 12
; Number of attention heads in each Transformer layer
num_heads = 8
; Whether to include bias in the linear transformations
bias = True
; Dimension of word embeddings
emb_dim = 512
; Dropout rate for regularization
dropout = 0.5
; Whether there is a special start token in input sequences
start_token = True

[training_parameters]
; Number of samples per batch
batch_size = 32
; Maximum number of epochs for training (if early stopping is not triggered)
epochs = 200
; Stop training if no improvement in validation metrics for this many epochs
early_stopping_trigger = 30
; Epoch to start training from (useful for resuming training)
start_epoch = 0
; Learning rate for the encoder
encoder_lr = 4e-4
; Learning rate for the decoder
decoder_lr = 1e-4
; Maximum absolute value for gradient clipping
grad_clip = 5.
; Frequency of printing training/validation stats (in batches)
print_freq = 1500
; Number of top predictions to consider during evaluation
top_k = 1
; Enable sanity checks for debugging purposes
sanity_check = True
; Normalize the dataset (if True)
normalize_ds = False
; Whether to remove the start token from sequences (if exists)
remove_start_token = False
; Adjust learning rate after this many epochs with no improvement
adjust_learning_rate_after = 8
; Whether to enable debug mode, which will stop training after one epoch for faster processing
debug_mode = False
