[file_names_and_paths]
; Output model filename
model_name = DenseNet121_LSTM
; Path to the dataset folder (containing the h5py files)
dataset_folder = data/ptb-xl/official_splits

[encoder_parameters]
; Number of blocks in the encoder (use 121 for DenseNet121)
num_blocks = 121
; Size of the encoded ECG feature vector (will be aggregated by attention layer)
encoded_ecg_size = 512
; Kernel sizes for the convolutional layers
kernel_size = [9, 7, 7, 5]
; Kernel size for the initial stem layer
stem_kernel_size = 9

[decoder_parameters]
; Dimension of the word embeddings
emb_dim = 512
; Dimension of attention linear layers
attention_dim = 512
; Dimension of the encoder output
encoder_dim = 156
; Dimension of the decoder
decoder_dim = 32
; Dropout rate for regularization
dropout = 0.5
; Number of layers in the decoder
layers = 1

[training_parameters]
; Maximum allowed length for generated captions
max_caption_length = 300
; Maximum number of epochs for training
epochs = 200
; Number of samples per batch
batch_size = 32
; Learning rate for the encoder
encoder_lr = 1e-4
; Learning rate for the decoder
decoder_lr = 4e-4
; Maximum absolute value for gradient clipping
grad_clip = 5.
; Regularization parameter for 'doubly stochastic attention'
alpha_c = 1.
; Frequency of printing training/validation statistics
print_freq = 300
; Path to a checkpoint file (set to None if not resuming training)
checkpoint = None
; Number of epochs with no improvement before stopping early
early_stopping_trigger = 30
; Probability of using teacher forcing during training
train_teacher_forcing_probability = 1
; Whether to use teacher forcing during validation
val_teacher_forcing = False
; Adjust learning rate after this many epochs of no improvement
adjust_learning_rate_after = 8
; Whether to run a sanity check (debugging/testing mode)
sanity_check = False
; Whether to normalize the dataset
normalize_ds = False
; Whether to enable debug mode, which short-circuits training 
debug_mode = False
